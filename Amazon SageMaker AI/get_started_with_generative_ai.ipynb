{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c55ee8-c18f-4fbe-92f9-eab02bb58e57",
   "metadata": {},
   "source": [
    "# Get Started with Generative AI\n",
    "\n",
    "Generative AI is a type of artificial intelligence that can create new content and ideas, including conversations, stories, images, videos, and music. Like all artificial intelligence, generative AI is powered by machine learning models—very large models that are pre-trained on vast amounts of data and commonly referred to as foundation models (FMs). Apart from content creation, generative AI is also used to improve the quality of digital images, edit video, build prototypes quickly for manufacturing, augment data with synthetic datasets, and more.\n",
    "\n",
    "## Using DeepSeek models \n",
    "\n",
    "---\n",
    "This demo notebook shows how to interact with a deployed DeepSeek model endpoint on Amazon SageMaker AI by using the SageMaker Python SDK for text generation. DeepSeek models are known for their strong performance, particularly in coding and reasoning tasks. We show several example prompt engineering use cases, including code generation, question answering, and controlled model output.\n",
    "\n",
    "Note: This notebook assumes you've already deployed a DeepSeek model to a SageMaker endpoint. You connect to this existing endpoint.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf04a9b-2da8-45ae-9346-4697937318a7",
   "metadata": {},
   "source": [
    "### Model details\n",
    "\n",
    "---\n",
    "DeepSeek LLM is a family of models developed by DeepSeek AI. The models come in various sizes and are trained on large datasets, with a significant portion dedicated to code, making them adept at programming-related tasks. \n",
    "\n",
    "This notebook focuses on interacting with a predeployed endpoint. For details on the specific DeepSeek model version, training data, and potential limitations (such as language support or inherent biases), refer to the model's documentation or the SageMaker JumpStart page used for its deployment.\n",
    "\n",
    "DeepSeek models often include the following characteristics:\n",
    "- Provide strong coding and mathematical reasoning capabilities\n",
    "- Available in base and instruction-tuned/chat variants\n",
    "- Trained on a diverse datasets, including web text and code\n",
    "\n",
    "DeepSeek models include the following limitations:\n",
    "- Like most large language models (LLMs), DeepSeek models can inherit biases from their training data. Use guardrails and appropriate precautions for production use.\n",
    "- Performance can vary across different languages or highly specialized domains not well-represented in the training data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be338dbb-57e7-434e-85d3-1427f5975f96",
   "metadata": {},
   "source": [
    "## Connect to the deployed DeepSeek endpoint\n",
    "Instead of deploying a model in this practice lab, you connect to an existing SageMaker endpoint that hosts the DeepSeek model. You need to provide the name of your specific endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3763cc4c-modified",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.predictor import retrieve_default\n",
    "import json \n",
    "import boto3\n",
    "\n",
    "# --- IMPORTANT --- \n",
    "# Replace this with the actual name of the deployed DeepSeek endpoint\n",
    "endpoint_name = \"<ENDPOINT_NAME>\" \n",
    "# ----------------- \n",
    "\n",
    "predictor = None # Initialize predictor\n",
    "try:\n",
    "    print(f\"Connecting to endpoint: {endpoint_name}...\")\n",
    "    # Use retrieve_default which automatically handles serializers/deserializers for known JumpStart containers\n",
    "    predictor = retrieve_default(endpoint_name)\n",
    "    print(f\"Successfully connected to endpoint: {endpoint_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"[Error] connecting to endpoint {endpoint_name}: {e}\")\n",
    "    print(\"Please ensure the endpoint name is correct and the endpoint is in 'InService' status.\")\n",
    "    # Optionally raise the error or handle it as needed\n",
    "    # raise e \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0286aa2-9c12-44a2-941f-1f1d808890c4",
   "metadata": {},
   "source": [
    "### Supported parameters (example for DeepSeek chat)\n",
    "\n",
    "DeepSeek models deployed through SageMaker AI often accept parameters within a JSON payload. The exact parameters depend on the specific deployment container (for example, TGI, vLLM, and DJL). Based on the reference code, a common structure for chat/instruct models involves a `messages` list and other generation controls. Common parameters include:\n",
    "\n",
    "* **messages:** A list of message objects, each with `role` ('user', 'assistant', or sometimes 'system') and `content` (the text of the message). This allows for conversational context.\n",
    "* **max_tokens** (or `max_new_tokens`): Maximum number of tokens to generate in the response.\n",
    "* **temperature:** Controls randomness. Lower values (0.1-0.3) make the output more focused and deterministic; higher values (0.7-1.0) make it more creative and diverse.\n",
    "* **top_p:** Nucleus sampling parameter. Considers only the most probable tokens whose cumulative probability exceeds `top_p`.\n",
    "* **stop:** A list of strings. Generation stops if the model produces any of these strings.\n",
    "\n",
    "Refer to the documentation of your specific SageMaker AI deployment (for example, TGI container images or the SageMaker JumpStart model card) for the definitive list of supported parameters and payload structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97bd778-5c62-4757-80ce-38c29275fa2a",
   "metadata": {},
   "source": [
    "## Create a query endpoint function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8eb4f7-65b8-428e-8d4f-ca8849228d56-modified",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Assume 'predictor' object and 'endpoint_name' variable exist from previous cells\n",
    "\n",
    "def query_endpoint(prompt, temperature=0.7, max_tokens=10240):\n",
    "    \"\"\"\n",
    "    This function handles sending the request, parsing the response to find \n",
    "    both the model's reasoning (thought process) and the final answer, \n",
    "    and prints them separately for clarity.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to send to the model.\n",
    "        temperature (float): Controls the randomness of the output (0.0 to ~1.0).\n",
    "                             Lower values are more focused, higher are more creative.\n",
    "        max_tokens (int): The maximum number of tokens (words/subwords) the model\n",
    "                          should generate in its response.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: The full response dictionary from the endpoint, or None if an error occurred.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n--> Sending prompt to endpoint: {endpoint_name}\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "    # Show the input prompt clearly\n",
    "    print(f\"[Input Prompt]:\\n{prompt}\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "\n",
    "    # 1. Prepare the data payload (the 'package' you send to the model)\n",
    "    #    DeepSeek chat models expect data in a specific format:\n",
    "    #    - A 'messages' list containing one or more message dictionaries.\n",
    "    #    - Each message dictionary has a 'role' ('user' for us, 'assistant' for the model)\n",
    "    #      and 'content' (the actual text of the message).\n",
    "    #    - You also include parameters such as 'max_tokens' and 'temperature'.\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt} # Our prompt goes here\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,         # Max length of the model's reply\n",
    "        \"temperature\": temperature      # Controls creativity compared to focus\n",
    "        # Other parameters, such as 'top_p', could be added here if needed\n",
    "    }\n",
    "\n",
    "    # Initialize variables to store parts of the response later\n",
    "    raw_response = None     # To store the entire response dictionary\n",
    "    reasoning = None        # To store the model's thought process\n",
    "    final_answer = None     # To store the model's final answer\n",
    "    warning_message = None  # To store any warnings (like truncation)\n",
    "\n",
    "    try:\n",
    "        # 2. Send the payload to the SageMaker endpoint by using the 'predictor'\n",
    "        #    The 'predictor.predict()' function handles the communication.\n",
    "        #    We need to make sure the 'predictor' object was successfully created earlier.\n",
    "        if 'predictor' not in globals() or predictor is None:\n",
    "             print(\"[Error]: 'predictor' object not found. \\nDid the connection to the endpoint in the previous cell succeed?\")\n",
    "             return None # Stop the function here if predictor doesn't exist\n",
    "             \n",
    "        print(\"Waiting for response from the model...\")\n",
    "        raw_response = predictor.predict(payload)\n",
    "\n",
    "        # 3. Process the response received from the endpoint\n",
    "        #    The response is usually a dictionary that contains details about the generation.\n",
    "        #    You need to carefully look inside it to find the generated text.\n",
    "\n",
    "        # Use '.get(key)' which is safer than '[key]' as it returns None if the key doesn't exist,\n",
    "        # preventing errors if the response structure is slightly different.\n",
    "        if isinstance(raw_response, dict):\n",
    "            choices = raw_response.get('choices')\n",
    "            # Check if 'choices' exists and is a list with at least one item\n",
    "            if isinstance(choices, list) and len(choices) > 0:\n",
    "                first_choice = choices[0] # Get the first (usually only) choice\n",
    "                if isinstance(first_choice, dict):\n",
    "                    message = first_choice.get('message') # Get the 'message' dictionary\n",
    "                    if isinstance(message, dict):\n",
    "                        # Try to get the model's reasoning (thought process)\n",
    "                        reasoning = message.get('reasoning_content')\n",
    "                        # Try to get the final answer/content\n",
    "                        final_answer = message.get('content')\n",
    "\n",
    "                    # Check if the response was cut short (truncated)\n",
    "                    finish_reason = first_choice.get('finish_reason')\n",
    "                    if finish_reason == 'length':\n",
    "                        warning_message = f\"[Warning]: The model's output might have been cut short because it reached the maximum token limit ({max_tokens}). You might need to increase 'max_tokens' for a longer response.\"\n",
    "        \n",
    "        # --- If you couldn't find the expected text parts --- \n",
    "        if final_answer is None and reasoning is None:\n",
    "             # Append to existing warning or create new one\n",
    "             if warning_message:\n",
    "                 warning_message += \"\\n[Warning]: Could not extract text using keys 'reasoning_content' or 'content'.\"\n",
    "             else:\n",
    "                 warning_message = \"[Warning]: Could not extract text using keys 'reasoning_content' or 'content'.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle errors that might happen during the prediction \n",
    "        # (for example, network problems, errors from the endpoint itself)\n",
    "        print(f\"\\n[Error] Occurred while querying the endpoint: {e}\")\n",
    "        try:\n",
    "             # Show the data you tried to send (helps find problems)\n",
    "             print(f\"   Payload attempted: {json.dumps(payload)}\")\n",
    "        except TypeError:\n",
    "             print(\"   Payload attempted: (Contains non-serializable types)\")\n",
    "        if raw_response is not None:\n",
    "             # Show the raw response if you received anything before the error\n",
    "             print(f\"   Raw response received before error: {raw_response}\")\n",
    "        return None # Indicate function failed by returning None\n",
    "\n",
    "    # 4. Print the results in a structured and easy-to-read way\n",
    "\n",
    "    print(\"\\n<-- Received response:\")\n",
    "    print(\"====================================================\")\n",
    "\n",
    "    # Print any warnings first\n",
    "    if warning_message:\n",
    "        print(f\"{warning_message}\\n\")\n",
    "\n",
    "    # Print the reasoning/thought process if it was found\n",
    "    # Check if reasoning is not None and is not empty string\n",
    "    if reasoning and reasoning.strip():\n",
    "        print(\"[Model's Reasoning (Thought Process)]:\")\n",
    "        print(reasoning.strip()) # .strip() removes leading/trailing whitespace\n",
    "        print(\"----------------------------------------------------\")\n",
    "    else:\n",
    "        # Inform the user if reasoning wasn't found or was empty\n",
    "        print(\"(No 'reasoning_content' found or it was empty in the response)\")\n",
    "        print(\"----------------------------------------------------\")\n",
    "\n",
    "    # Print the final answer if it was found\n",
    "    # Check if final_answer is not None and is not empty string\n",
    "    if final_answer and final_answer.strip():\n",
    "        print(\"[Final Answer]:\")\n",
    "        print(final_answer.strip())\n",
    "        print(\"====================================================\")\n",
    "    else:\n",
    "        # Inform the user if the final answer wasn't found or was empty\n",
    "        print(\"[Error] (No final 'content' found or it was empty in the response)\")\n",
    "        print(\"====================================================\")\n",
    "        # If *both* parts were effectively missing, show the raw response to help debug\n",
    "        if not (reasoning and reasoning.strip()):\n",
    "             print(\"\\nRaw Response Received (for debugging, as key content seems missing):\")\n",
    "             # Pretty print the JSON for better readability\n",
    "             try:\n",
    "                 print(json.dumps(raw_response, indent=2))\n",
    "             except Exception:\n",
    "                 print(raw_response) # Fallback if JSON formatting fails\n",
    "\n",
    "    # 5. Return the complete raw response dictionary\n",
    "    #    This allows the code calling this function to inspect other details\n",
    "    #    such as token usage ('usage' dictionary) if needed.\n",
    "    return raw_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a9caa-5dfd-4a34-a56f-469468d0ba44",
   "metadata": {},
   "source": [
    "# Prompt engineering techniques\n",
    "\n",
    "Now, it's time to test the same prompt engineering techniques by using the connected DeepSeek model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4585b9f-eb15-4f8c-bffe-d1807f0f66b3",
   "metadata": {},
   "source": [
    "## Zero-shot prompting\n",
    "In zero-shot prompting, you ask the model to perform a task without any examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3d263-f600-4354-bb04-f117b0e55440",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = \"\"\"\n",
    "Write a program to compute factorial in Python.\n",
    "\"\"\"\n",
    "# Use the updated query function\n",
    "raw_response = query_endpoint(zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b17a2-aeee-45af-8c47-a3896a0e67b6",
   "metadata": {},
   "source": [
    "## One-shot prompting\n",
    "In one-shot prompting, you provide one example to guide the model. \n",
    "\n",
    "Note: For chat models such as DeepSeek, structuring this example within the `messages` list as a user/assistant pair might be more effective than including it directly in the user prompt, but the original structure is kept in this practice lab for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72a938-3704-4bc4-98fa-3cf87d7a8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_prompt = \"\"\"\n",
    "Here's an example of an AWS Lambda function that generates weather forecasts:\n",
    "\n",
    "```python\n",
    "import json\n",
    "import random\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        weather_types = [\"sunny\", \"rainy\", \"cloudy\"]\n",
    "        forecast = random.choice(weather_types)\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps({'forecast': forecast})\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps({'error': str(e)})\n",
    "        }\n",
    "```\n",
    "\n",
    "Now, write a similar Lambda function that generates random colors (e.g., red, green, blue).\n",
    "\"\"\"\n",
    "# Use the updated query function\n",
    "raw_response =  query_endpoint(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23902acc-ca1e-4af1-9e94-f63be5bd00cc",
   "metadata": {},
   "source": [
    "## Few-shot prompting\n",
    "In few-shot prompting, you provide multiple examples to establish a pattern.\n",
    "\n",
    "Note: Similar to one-shot prompting, providing these examples as distinct user/assistant turns in the `messages` list is often better for chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900b065-aa54-414a-9ba1-6dc5ff47d71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = \"\"\"\n",
    "Example 1 - Simple Lambda:\n",
    "```python\n",
    "def lambda_handler(event, context):\n",
    "    return {'message': 'Hello World'}\n",
    "```\n",
    "\n",
    "Example 2 - Lambda with error handling:\n",
    "```python\n",
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        # Your logic here\n",
    "        result = 'Success'\n",
    "        return {'statusCode': 200, 'body': json.dumps({'message': result})}\n",
    "    except Exception as e:\n",
    "        return {'statusCode': 500, 'body': json.dumps({'error': str(e)})}\n",
    "```\n",
    "\n",
    "Example 3 - Lambda with logging:\n",
    "```python\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    logger.info(f'Processing event: {event}')\n",
    "    try:\n",
    "        # Your logic here\n",
    "        result = 'Logged and Processed'\n",
    "        return {'statusCode': 200, 'body': json.dumps({'message': result})}\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error processing event: {e}')\n",
    "        return {'statusCode': 500, 'body': json.dumps({'error': str(e)})}\n",
    "```\n",
    "\n",
    "Now, create a Lambda function that:\n",
    "1. Imports necessary libraries (random, logging, json)\n",
    "2. Includes proper error handling (try/except block)\n",
    "3. Implements logging for requests and errors\n",
    "4. Defines a list of quotes from Greek philosophers (e.g., Socrates, Plato, Aristotle)\n",
    "5. Randomly selects and returns one quote in the JSON body\n",
    "6. Includes basic docstrings and comments\n",
    "7. Returns a 200 status code on success and 500 on error.\n",
    "\"\"\"\n",
    "\n",
    "# Use the updated query function, keeping max_tokens high for longer code\n",
    "# Increased max_tokens slightly more just in case\n",
    "raw_response = query_endpoint(few_shot_prompt, max_tokens=1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85354676-4ae1-4063-b5e4-0eb245e95155",
   "metadata": {},
   "source": [
    "## Use clear and specific instructions\n",
    "\n",
    "Prompt engineering techniques—such as zero-shot, one-shot, and few-shot approaches—offer different ways to guide AI models to produce desired outputs. Zero-shot prompting is the most basic approach, where you directly ask the model to perform a task without examples. Zero-shot prompting is useful for straightforward requests, but it might lack precision for complex tasks. One-shot prompting provides a single example along with the request, which helps the model understand the expected format and style of the response. Few-shot prompting takes this further by providing multiple examples, which is particularly effective for complex tasks that require pattern recognition or specific output structures.\n",
    "\n",
    "Clear instructions are crucial across all these techniques because they reduce ambiguity and help the model understand exactly what's expected. When instructions are specific, well-structured, and include details about the desired format, context, and constraints, the model is more likely to generate accurate and relevant responses. For example, instead of asking the model to \"generate some code,\" a clear instruction might specify that the model should \"write a Python function that handles errors, includes logging, and returns JSON responses with specific fields.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694cb4f-2672-4af7-9613-dc671e71024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_prompt = \"\"\"\n",
    "Create an AWS Lambda function in Python with the following specific requirements:\n",
    "1.  Function name should ideally be `greek_philosopher_quote_lambda` (inside the handler code).\n",
    "2.  It must include a list containing quotes from at least these three philosophers: Socrates, Plato, Aristotle.\n",
    "3.  Each quote in the list should ideally be stored as a dictionary or tuple, pairing the quote text with the philosopher's name (e.g., {'philosopher': 'Socrates', 'quote': 'An unexamined life is not worth living.'}).\n",
    "4.  The function should randomly select one quote object (philosopher and quote) from the list.\n",
    "5.  It must include error handling using a try/except block to catch potential issues during execution.\n",
    "6.  It must implement logging using the `logging` library to log the start of the function execution and any errors encountered.\n",
    "7.  The function must return a dictionary suitable for API Gateway proxy integration, specifically:\n",
    "    - On success: `{'statusCode': 200, 'body': json.dumps({'philosopher': selected_philosopher, 'quote': selected_quote})}`\n",
    "    - On error: `{'statusCode': 500, 'body': json.dumps({'error': 'An error occurred'})}` or a more specific error message.\n",
    "8. Include basic docstrings for the function and comments where necessary.\n",
    "\"\"\"\n",
    "\n",
    "# Use the updated query function, keeping max_tokens high\n",
    "raw_response = query_endpoint(clear_prompt, max_tokens=1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c81e8a",
   "metadata": {},
   "source": [
    "# Temperature and creativity control\n",
    "\n",
    "Temperature in prompt engineering controls the randomness or predictability of a model's responses. A lower temperature (closer to 0) makes responses more deterministic and focused, which is better for factual or technical tasks where accuracy is crucial. A higher temperature (closer to 1 or above) introduces more randomness, leading to more creative, diverse, and surprising outputs, which works well for creative writing or brainstorming. The optimal temperature setting depends on your specific use case; use lower values when you need consistency and precision, and use higher values when you want variety and creativity.\n",
    "\n",
    "### Controlling model creativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec221d6e-modified",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"What is AWS Lambda?\"\n",
    "\n",
    "# Low temperature (more focused, deterministic)\n",
    "print(\"\\n--- Low Temperature (Focused, temp=0.2) Response --- \")\n",
    "raw_response = query_endpoint(prompt, temperature=0.2, max_tokens=32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e182d8-modified",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Write a haiku about the challenges of software development and code maintenance.\"\n",
    "\n",
    "# Higher temperature (more creative, diverse)\n",
    "print(\"\\n--- High Temperature (Creative, temp=0.9) Response --- \")\n",
    "raw_response = query_endpoint(prompt, temperature=0.9, max_tokens=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f438f741",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "Important: Before proceding to the DIY section of this solution, delete the deployed model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b28586-3e8b-4479-9e7e-ff877419fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try:\n",
    "    sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"Successfully deleted endpoint: {endpoint_name}\")\n",
    "except ClientError as e:\n",
    "    print(f\"Failed to delete endpoint {endpoint_name}: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2c620-c210-4cc2-a73b-f1538bab0806",
   "metadata": {},
   "source": [
    "# Do it yourself \n",
    "\n",
    "Stop here and deploy a new model as part of the DIY section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9856ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "def create_predictor(endpoint_name, session=None):\n",
    "    \"\"\"\n",
    "    Create a predictor for an existing SageMaker JumpStart LLaMA 3 endpoint\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    endpoint_name : str\n",
    "        The name of the existing SageMaker endpoint\n",
    "    session : sagemaker.session.Session, optional\n",
    "        SageMaker session to use. If not provided, a new one will be created\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    predictor : sagemaker.predictor.Predictor\n",
    "        A predictor object that can be used to make text generation requests\n",
    "    \"\"\"\n",
    "    if session is None:\n",
    "        boto_session = boto3.Session()\n",
    "        session = sagemaker.Session(boto_session=boto_session)\n",
    "    \n",
    "    # Create a predictor with proper configuration\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=endpoint_name,\n",
    "        sagemaker_session=session,\n",
    "        serializer=JSONSerializer(),\n",
    "        deserializer=JSONDeserializer()\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(f\"Successfully connected to LLaMA 3 endpoint: {endpoint_name}\")\n",
    "    return predictor\n",
    "\n",
    "def format_llama3_prompt(instruction, examples=None):\n",
    "    \"\"\"\n",
    "    Format a prompt for LLaMA 3 following Meta's recommended format\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    instruction : str\n",
    "        The main instruction or question\n",
    "    examples : list of dict, optional\n",
    "        List of examples for one-shot or few-shot learning\n",
    "        Each example should be a dict with 'user' and 'assistant' keys\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    formatted_prompt : str\n",
    "        Properly formatted prompt for LLaMA 3\n",
    "    \"\"\"\n",
    "    # Base system prompt that works well with LLaMA 3\n",
    "    system_prompt = \"You are a helpful, harmless, and honest AI assistant.\"\n",
    "    \n",
    "    # Start with the system prompt\n",
    "    formatted_prompt = f\"<|system|>\\n{system_prompt}\\n<|end|>\\n\"\n",
    "    \n",
    "    # Add examples for one-shot or few-shot learning\n",
    "    if examples:\n",
    "        for example in examples:\n",
    "            formatted_prompt += f\"<|user|>\\n{example['user']}\\n<|end|>\\n\"\n",
    "            formatted_prompt += f\"<|assistant|>\\n{example['assistant']}\\n<|end|>\\n\"\n",
    "    \n",
    "    # Add the current instruction\n",
    "    formatted_prompt += f\"<|user|>\\n{instruction}\\n<|end|>\\n\"\n",
    "    formatted_prompt += \"<|assistant|>\\n\"\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "def generate_text(predictor, instruction, examples=None, max_new_tokens=4096, temperature=0.3, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate text by using the LLaMA 3 model with proper prompt formatting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictor : sagemaker.predictor.Predictor\n",
    "        The predictor object for the endpoint\n",
    "    instruction : str\n",
    "        The main instruction or question\n",
    "    examples : list of dict, optional\n",
    "        List of examples for one-shot or few-shot learning\n",
    "        Each example should be a dict with 'user' and 'assistant' keys\n",
    "    max_new_tokens : int\n",
    "        Maximum number of tokens to generate\n",
    "    temperature : float\n",
    "        Temperature for sampling (higher = more creative)\n",
    "    top_p : float\n",
    "        Top-p sampling parameter\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    response : dict\n",
    "        The complete response from the model\n",
    "    \"\"\"\n",
    "    # Format the prompt according to LLaMA 3 requirements\n",
    "    formatted_prompt = format_llama3_prompt(instruction, examples)\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": formatted_prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"return_full_text\": False,\n",
    "            \"stop\": [\"<|end|>\", \"</s>\"]  # Stop tokens for LLaMA 3\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = predictor.predict(payload)\n",
    "        print(f\"\\033[1m Input Instruction:\\033[0m {instruction}\")\n",
    "        \n",
    "        # Extract and clean the generated text\n",
    "        if isinstance(response, dict) and \"generated_text\" in response:\n",
    "            generated_text = response[\"generated_text\"]\n",
    "            # Clean any trailing stop tokens that might have been included\n",
    "            for stop_token in [\"<|end|>\", \"</s>\"]:\n",
    "                if generated_text.endswith(stop_token):\n",
    "                    generated_text = generated_text[:-len(stop_token)].strip()\n",
    "            \n",
    "            print(f\"\\033[1m Output:\\033[0m {generated_text}\")\n",
    "            return {\"generated_text\": generated_text}\n",
    "        else:\n",
    "            # Handle different response formats\n",
    "            print(f\"\\033[1m Output:\\033[0m {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c20b888",
   "metadata": {},
   "source": [
    "### Your turn, using the Meta LLAMA 3 model!\n",
    "\n",
    "Create a prompt that generates an AWS Lambda function by using the **Llama 3** model with the following features:\n",
    "\n",
    "1. Generates quotes from a specific philosopher of your choice\n",
    "2. Includes sentiment analysis of the quote\n",
    "3. Returns both the quote and its sentiment score\n",
    "4. Implements proper error handling and logging\n",
    "\n",
    "In the next cell, enter your prompt and experiment with different prompting techniques for Llama 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83057a7b-4ded-40ab-96ce-f5d98f252c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example solution prompt (update the following prompt)\n",
    "\n",
    "diy_prompt = \"\"\"\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ff4b4-fded-4029-8f1f-312196a0a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your specific endpoint name\n",
    "endpoint_name = \"<DIY_ENDPOINT>\"\n",
    "\n",
    "# Create the predictor\n",
    "predictor = create_predictor(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1472c-3d4d-4024-a4e5-9017ce4a8cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_response = generate_text(predictor,diy_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
